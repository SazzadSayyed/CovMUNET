{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiloss.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tByYyAcnF_eu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f03c0b2b-4396-48cb-ae6f-b44908b558b0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGwkp9DNGF2V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa5fe4aa-800f-4e4c-c219-ab11a569c2e5"
      },
      "source": [
        "\n",
        "# General libraries\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd \n",
        "import random\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Deep learning libraries\n",
        "import keras.backend as K\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, BatchNormalization\n",
        "from keras.layers import Conv2D, SeparableConv2D, MaxPooling2D, LeakyReLU, Activation, Lambda, GlobalAveragePooling2D, DepthwiseConv2D, GlobalMaxPooling2D, Conv2DTranspose\n",
        "from keras.layers import Add, Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras.optimizers import Adam, SGD\n",
        "\n",
        "# Setting seeds for reproducibility\n",
        "seed = 232\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gC8KKuPQGHBf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_path = '/content/drive/My Drive/COVID DATA/fold1/' #You have to change Fold Name Here..................."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsNacmuXGLkc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Preprocessing and DataGenerator definition\n",
        "#cond = 'COVID19'\n",
        "class_name = ['/covid/', '/normal/', '/pneumonia/']\n",
        "#class_name = [/normal/', '/pneumonia/']\n",
        "\n",
        "def process_data(img_dims):\n",
        "\n",
        "    X_train = []  \n",
        "    y_train = []\n",
        "\n",
        "    for cls in class_name:\n",
        "      for img_name in (os.listdir(input_path + 'train' + cls)):\n",
        "        img = cv2.imread(input_path + 'train' + cls + img_name, 0)\n",
        "        try:\n",
        "          img = cv2.resize(img, (img_dims, img_dims))\n",
        "        except:\n",
        "          print(img_name)\n",
        "        X_train.append(img/255.0)\n",
        "\n",
        "        if cls=='/covid/':\n",
        "          y_train.append(0)\n",
        "        elif cls=='/normal/':\n",
        "          y_train.append(1)\n",
        "        elif cls=='/pneumonia/':\n",
        "          y_train.append(1)\n",
        "\n",
        "    # I will be making predictions off of the test set in one batch size\n",
        "    # This is useful to be able to get the confusion matrix\n",
        "    X_test = []\n",
        "    y_test = []\n",
        "\n",
        "    for cls in class_name:\n",
        "      for img_name in (os.listdir(input_path + 'test' + cls)):\n",
        "          img = cv2.imread(input_path+'test' + cls + img_name, 0) #We are taking image in grayscale form. \n",
        "          try:\n",
        "            img = cv2.resize(img, (img_dims, img_dims))\n",
        "          except:\n",
        "            print(img_name)\n",
        "          X_test.append(img/255.0)\n",
        "\n",
        "          if cls=='/covid/':\n",
        "            y_test.append(0)\n",
        "          elif cls=='/normal/':\n",
        "            y_test.append(1)\n",
        "          elif cls=='/pneumonia/':\n",
        "            y_test.append(1)\n",
        "        \n",
        "    X_test = np.array(X_test)\n",
        "    X_train = np.array(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "    if len(np.unique(y_train))==2:\n",
        "      y_train = y_train - 1\n",
        "      y_test = y_test - 1\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2vAzvlIGJgH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "img_dims = 128\n",
        "epochs = 15\n",
        "batch_size = 32\n",
        "X_train, X_test, y_train, y_test = process_data(img_dims)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy-mcFZXGNaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_block(input, chs, regularizer=False): ## Convolution block of 2 layers\n",
        "    x = SeparableConv2D(chs, 3, padding=\"same\")(input)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    return x\n",
        "\n",
        "def unet():  \n",
        "    input = Input((img_dims,img_dims,1))\n",
        "    \n",
        "    # Encoder\n",
        "    block1 = create_block(input, 32)\n",
        "    x = MaxPooling2D(2)(block1)\n",
        "    block2 = create_block(x, 64)\n",
        "    x = MaxPooling2D(2)(block2)\n",
        "    block3 = create_block(x, 128)\n",
        "    x = MaxPooling2D(2)(block3)\n",
        "    block4 = create_block(x, 256)\n",
        "    \n",
        "    # Middle\n",
        "    x = MaxPooling2D(2)(block4)\n",
        "    middle = create_block(x, 512)\n",
        "    \n",
        "    # Decoder\n",
        "    x = Conv2DTranspose(256, kernel_size=2, strides=2)(middle)\n",
        "    x = Concatenate()([block4, x])\n",
        "    x = create_block(x, 128)\n",
        "    x = Conv2DTranspose(64, kernel_size=2, strides=2)(x)\n",
        "    x = Concatenate()([block3, x])\n",
        "    x = create_block(x, 64)\n",
        "    x = Conv2DTranspose(64, kernel_size=2, strides=2)(x)\n",
        "    x = Concatenate()([block2, x])\n",
        "    x = create_block(x, 64)\n",
        "    x = Conv2DTranspose(32, kernel_size=2, strides=2)(x)\n",
        "    x = Concatenate()([block1, x])\n",
        "    x = create_block(x, 32)\n",
        "   \n",
        "    # output\n",
        "    output = Conv2D(1, 1, activation='sigmoid', name='autoencoder_output')(x)\n",
        "\n",
        "    middle_output = GlobalAveragePooling2D(name='concatenate1')(middle)\n",
        "    y1 = Dense(512, activation='relu', name='concatenate2')(middle_output)\n",
        "    y = Dense(2, activation='softmax', name='classification_output')(y1)\n",
        "    \n",
        "    return Model(input, [output, y]), Model(input, y1)\n",
        "\n",
        "\n",
        "#encoder_model, autoencoder_model = unet()\n",
        "\n",
        "model, model2 = unet()\n",
        "\n",
        "losses = {'classification_output':'categorical_crossentropy', 'autoencoder_output':'mse'}\n",
        "lossWeights = {'classification_output':1.0, 'autoencoder_output':1.0}\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss=losses, loss_weights=lossWeights, metrics={'classification_output': 'accuracy', 'autoencoder_output': 'mse'})\n",
        "#model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGiEDcxvNqJN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae546188-dcc9-440e-a0af-d953df6a8ff7"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], -1, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], -1, 1)\n",
        "print(X_train.shape, X_test.shape)\n",
        "from keras.utils import to_categorical\n",
        "y_train_cat = to_categorical(y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5012, 128, 128, 1) (1255, 128, 128, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27h6vjlcGPUM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_name = 'MultiLoss_1.0Fold1' #Change it as necessary, This is the base name for model\n",
        "\n",
        "weight_save_path = '/content/drive/My Drive/Results/Weights/' #..................................................Change path as necessary\n",
        "\n",
        "lr_reduce = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=2, mode='auto', min_lr=0.000001)\n",
        "#early_stop = EarlyStopping(monitor='val_loss', min_delta=0.001, patience=3, mode='min')\n",
        "checkpoint = ModelCheckpoint(weight_save_path+model_name+'.h5', monitor='classification_output_accuracy', save_best_only=True, save_weights_only=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxxDXxl6GRHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fitting the model \n",
        "hist = model.fit(\n",
        "           x=X_train, y={'classification_output':y_train_cat, 'autoencoder_output':X_train}, epochs=epochs, batch_size=batch_size, validation_split=0.1, callbacks=[lr_reduce, checkpoint])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9Au2Ylbao5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.load_weights(weight_save_path+model_name+'.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6CRphBjGgRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "_,y_pred = model.predict(X_test)\n",
        "y_pred_bool = np.argmax(y_pred, axis=1)\n",
        "\n",
        "report = classification_report(y_test, y_pred_bool, output_dict=True)\n",
        "print(classification_report(y_test, y_pred_bool))\n",
        "\n",
        "result_df = report\n",
        "result_df = pd.DataFrame(result_df).transpose()\n",
        "\n",
        "print(result_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqPj6pSlptU1",
        "colab_type": "text"
      },
      "source": [
        "After this evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTn6OTxKHFWF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "############## Make Sure to Change this ####################\n",
        "plot_save_path = '/content/drive/My Drive/Results/ModelPlot/'#..................................................Change path as necessary\n",
        "hist_save_path = '/content/drive/My Drive/Results/History/' #.................................................. Change path as necessary\n",
        "result_save_path = '/content/drive/My Drive/Results/Result/' #..............................................Change path as necessary\n",
        "confusion_matrix_save_path = '/content/drive/My Drive/Results/ConfusionMatrix/' #.............................Change path as necessary\n",
        "############ According to the Path You are Using ##############\n",
        "\n",
        "\n",
        "hist_df = pd.DataFrame(hist.history)\n",
        "hist_csv_file = hist_save_path+model_name+'_history.csv'\n",
        "with open(hist_csv_file, mode='w') as f:\n",
        "    hist_df.to_csv(f)\n",
        "\n",
        "plot_model(model, plot_save_path+model_name+'.png', show_shapes=True)\n",
        "\n",
        "result_df = report\n",
        "result_df = pd.DataFrame(result_df).transpose()\n",
        "\n",
        "print(result_df)\n",
        "\n",
        "result_df.to_csv(result_save_path+model_name+'_result.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9v6tzB5BHHMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from seaborn import heatmap\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "preds = np.argmax(y_pred, axis=1)\n",
        "\n",
        "acc = accuracy_score(y_test, np.round(preds))*100\n",
        "cm = confusion_matrix(y_test, np.round(preds))\n",
        "cm_norm = confusion_matrix(y_test, np.round(preds), normalize='true')\n",
        "#tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "print('CONFUSION MATRIX ------------------')\n",
        "\n",
        "ax = heatmap(cm, cmap='Accent', annot=True, xticklabels=['COVID19', 'NORMAL', 'PNEUMONIA'], yticklabels=['COVID19', 'NORMAL', 'PNEUMONIA'], square=True, fmt='d')\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'.png')\n",
        "plt.show()\n",
        "ax = heatmap(cm_norm, cmap='Accent', annot=True, xticklabels=['COVID19', 'NORMAL', 'PNEUMONIA'], yticklabels=['COVID19', 'NORMAL', 'PNEUMONIA'], square=True, fmt='f')\n",
        "\n",
        "plt.savefig(confusion_matrix_save_path+model_name+'_normalized.png')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}